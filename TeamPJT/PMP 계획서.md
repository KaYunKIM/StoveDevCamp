## 목표

### 개인

- 추연호: 디자인 시스템

- 정석준: 웹 개발 경험이 없어서 하나의 완성된 팀 프로젝트 전체적인 흐름에 대한 개념 잡고 싶다.

- 이규은: 대용량 데이터 수집, 처리 접하고 싶었다=> 분산 처리 관리, 시스템 전체에 대한 것 파악 및 핸들링하는 것

- 조인식: 데이터 관심, 프레임워크 종류가 많은 거를 알고있어서 각자 장단점 명확히 파악 후 이번 프로젝트에 적용 => 이후에 공부했던 것 가지고 판단

  프레인워크 뿐 아니라 데이터파이프 라인 흐름 파악하기

- 김가윤: 데이터 처리에 관심 end-to-end 데이터 수집, 저장, 처리, 분석, 시가화까지 일련의 과정을 경험하고 기술적 스택을 쌓는 것



### 팀

- 애자일 방식
  - 작게 만들어서 빨리빨리 만들어보고, 이후 진행 상황 계획
- 안정적 서버
- 국내 vs 글로벌
- 애자일 방식으로 한 cycle 돌면서 피드백 하면서 수정해 나가기
- 기능 구현에 초점 맞추는 것 보다 기술 습득에 초점 맞추기
- 기술 구현을 위해 어떤 것을 배웠는지, 문서화하여 서로 공유하기
- 개인의 목표 다 이루기!
- 이후 4개월 기간 동안 배포+트래픽 늘리기



### 팀 빌딩

- 프론트: 추연호
- 백엔드: 정석준
- 데이터 처리: 조인식, 이규은, 김가윤



### milestone

1. TweetDeck clone 코딩

   - 프론트: 키워드, Trending UI
   - 데이터: 

2. 일정 계획표

   - 1-2주차: 12/31일 까지 명세서 확정짓고 작성하기

     프론트- 와이어프레임 기획

     백엔드- 소셜 로그인(구글, 카카오, 네이버)

     데이터- 트위터 API, 분산형 DB공부 및 적용, 기술 스택 확정

   - 3-4주차: 프론트- UI 완성, 백엔드- API 개발, 데이터- 백엔드와 연결하기

   - 중간발표

   - 5-6주차: 오류 수정, 배포 

   - 2월 5일 완료

3. Kafka: 실시간 받아오기 front 보내기: spark

   수집, 저장DB





## 기능 정의서

- 회원가입 + 로그인
  - 일반 회원가입
  - +@ 소셜 로그인(트위터 or 구글 + 카카오)
- 유저 관심 키워드 폼 제작
  - 2~3의 관심 키워드 입력을 받아서 유저별로 관리하고 탭으로 분리해서 화면에 보여준다.
- 한 화면에 그 키워드에 대한 트렌드를 분석한 뷰를 그린다.
- 워드 클라우드로 보여준다.
- 키워드 관련된 실시간 피드를 한 칼럼에서 보여준다.
- 해시태그 키워드
  1. 관련 키워드 빈도수(world cloud/ bubble chart)
  2. 시기별 검색 빈도수(그래프)
  3. 누가 가장 관심 있는 키워드인지
- 키워드 언급량 통계 그래프 (기간별)
- 키워드별 언급량 비교

### 

## 아키텍처 설계

### DB 테이블 및 스키마 (세부사항)

- User 테이블 ERD

```bash
CREATE TABLE `user` (
  `id` int(10) unsigned NOT NULL AUTO_INCREMENT,
  `profile` varchar(255) DEFAULT '',
  `name` varchar(255) NOT NULL,
  `confirmed` tinyint(1) NOT NULL DEFAULT '0',
  `email` varchar(255) NOT NULL,
  `password` varchar(255) NOT NULL,
  `salt` varchar(255) NOT NULL,
  `createdAt` datetime NOT NULL,
  `updatedAt` datetime NOT NULL,
  `accessedAt` datetime NOT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `email` (`email`)
) ENGINE=InnoDB AUTO_INCREMENT=5 DEFAULT CHARSET=utf8;
```



### API 명세서 (세부사항)

### UI/UX 설계 (세부사항)

- 와이어프레임 제작



## 담당 역할

- 프론트 → 추연호
  1. Web Socket을 활용한 실시간 피드 컬럼 개발
  2. 로그인, 회원가입 페이지 개발 + 인증 서버 개발(백엔드)
  3. 트렌드 데이터 시각화
  4. 컴포넌트 개발

- 백엔드 → 정석준
  1. 데이터 조회 및 프론트 전달
  2. 백엔드 기능 개발
  3. redis를 활용한 캐싱

- 데이터 처리

  1. 데이터 수집 및 분산 처리(카프카) → 조인식

  2. 분산 데이터 가공 및 전처리(스파크) → 김가윤

  3. 가공 데이터 분산 저장 및 전달(분산형 DB) → 이규은

     

## 마일스톤 (일정)

- 1-2주차: 12/31일 까지 명세서 확정짓고 작성하기

  프론트- 와이어프레임 기획

  백엔드- 소셜 로그인(구글, 카카오, 네이버)

  데이터- 트위터 API, 분산형 DB공부 및 적용, 기술 스택 확정

- 3-4주차: 프론트- UI 완성, 백엔드- API 개발, 데이터- 백엔드와 연결하기

- 중간발표

- 5-6주차: 오류 수정, 배포

- 2월 5일 완료

### m1 - 마감일: 12/31

- DB 데이터 API → MySQL 또는 MongoDB 등 자신 있는 DB 아무거나에 데이터 모아보기
  - Tweeter API로 코로나 관련 해시태그 모으기
- 디자인 프로토타입 제작
- 프론트, 백엔드, 데이터 연결해서 한 사이클 돌려보기(키워드 한 개로 잡기)
- 기능 확정 지어서 명세서 작성하기

### m2 - 마감일: 1월 12일 - 중간 발표

- 프론트엔드 디자인 시스템 제작
- MSA 구조 점검
- 인증기능 구현
- 데이터처리 부분과 백엔드 부분의 연동
- 특정 키워드에 대한 관련 해시 태그 정보 실시간 처리
- 처리된 데이터 시각화(샘플)

### m3 - 마감일: 1월 29일

- 데이터 처리 확장 (키워드 개수 확대)
- 데이터 분석을 위한 데이터 처리 시스템 구축
- 프론트 페이지 레이아웃 및 디자인 수정
- 성능향상

### m4 - 마감일: 2/5 - 최종 발표

- 확장된 키워드 및 분석을 위한 데이터 시각화
- 최종 기능 테스트
- 발표 PPT 자료 준비