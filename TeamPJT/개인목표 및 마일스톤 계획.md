#### 개인 목표

- 데이터 처리 중에서도 spark streaming을 활용한 실시간 데이터 가져오기 + DB 전달 기능 구현해보기
- spark에서 전처리 기능을 사용하여 kafka에서 가져온 raw data 가공하여 keywordAPI에 전달하기
- 이 두가지를 위해 kafka와 cassandra DB 사이에서 유실 없는(최대 20%의 유실) 데이터 파이프라인 구축하기
- 개인 목표 달성을 위한 리스크는 데이터 처리에 대한 경험 부족으로, spark를 찾아보면서 공부했던 내용들 gitlab 데이터팀 repo에 1일 1commit 하기
- 데이터팀 내 spark를 담당하지만, 앞단의 데이터 수집과 이후 DB저장까지 데이터 파이프라인의 전체적 흐름을 파악하기 위해, 2가지 담당 팀원들과 협업을 통해 어떻게 서로 연결하는지와 각자 어떻게 작동하는지에 대해 백엔드와 프론트 팀원들에게 자신있게 설명할 수 있을 정도로 정확히 숙지할 것



#### 마일스톤 계획

- M1 (1/8 금)
  1. 데이터 통합 처리를 위한 AWS서버 구축 및 환경설정하기
  2. spark streaming을 kafka consumer 서버로 연결해서 kafka에 저장되어 있는 data 가져오기
  3. cassandra DB와 연결하여 실시간으로 가져온 tweeter 피드를 보내기 
  4. 위 3가지를 구현하지 못했을 시 주말+중간발표(1/12)까지 날밤을 새서 끝내기 

- M2 (1/12 화)
  1. M1 완료를 1차 목표로 두고, 완료 시 spark에서 데이터 가공 시도해보기
  2. 데이터 가공 후 다시 cassandra DB에 저장 or keyword API로 통신하는 것 구현해보기

- M3 (1/29 금)

- M4 (2/5 금)

