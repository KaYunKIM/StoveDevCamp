{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://kyunkim-NB.smilegate.local:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.textFile(\"README.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['# Apache Spark',\n",
       " '',\n",
       " 'Spark is a unified analytics engine for large-scale data processing. It provides',\n",
       " 'high-level APIs in Scala, Java, Python, and R, and an optimized engine that',\n",
       " 'supports general computation graphs for data analysis. It also supports a',\n",
       " 'rich set of higher-level tools including Spark SQL for SQL and DataFrames,',\n",
       " 'MLlib for machine learning, GraphX for graph processing,',\n",
       " 'and Structured Streaming for stream processing.',\n",
       " '',\n",
       " '<https://spark.apache.org/>',\n",
       " '',\n",
       " '[![Jenkins Build](https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-sbt-hadoop-2.7-hive-2.3/badge/icon)](https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-sbt-hadoop-2.7-hive-2.3)',\n",
       " '[![AppVeyor Build](https://img.shields.io/appveyor/ci/ApacheSoftwareFoundation/spark/master.svg?style=plastic&logo=appveyor)](https://ci.appveyor.com/project/ApacheSoftwareFoundation/spark)',\n",
       " '[![PySpark Coverage](https://img.shields.io/badge/dynamic/xml.svg?label=pyspark%20coverage&url=https%3A%2F%2Fspark-test.github.io%2Fpyspark-coverage-site&query=%2Fhtml%2Fbody%2Fdiv%5B1%5D%2Fdiv%2Fh1%2Fspan&colorB=brightgreen&style=plastic)](https://spark-test.github.io/pyspark-coverage-site)',\n",
       " '',\n",
       " '',\n",
       " '## Online Documentation',\n",
       " '',\n",
       " 'You can find the latest Spark documentation, including a programming',\n",
       " 'guide, on the [project web page](https://spark.apache.org/documentation.html).',\n",
       " 'This README file only contains basic setup instructions.',\n",
       " '',\n",
       " '## Building Spark',\n",
       " '',\n",
       " 'Spark is built using [Apache Maven](https://maven.apache.org/).',\n",
       " 'To build Spark and its example programs, run:',\n",
       " '',\n",
       " '    ./build/mvn -DskipTests clean package',\n",
       " '',\n",
       " '(You do not need to do this if you downloaded a pre-built package.)',\n",
       " '',\n",
       " 'More detailed documentation is available from the project site, at',\n",
       " '[\"Building Spark\"](https://spark.apache.org/docs/latest/building-spark.html).',\n",
       " '',\n",
       " 'For general development tips, including info on developing Spark using an IDE, see [\"Useful Developer Tools\"](https://spark.apache.org/developer-tools.html).',\n",
       " '',\n",
       " '## Interactive Scala Shell',\n",
       " '',\n",
       " 'The easiest way to start using Spark is through the Scala shell:',\n",
       " '',\n",
       " '    ./bin/spark-shell',\n",
       " '',\n",
       " 'Try the following command, which should return 1,000,000,000:',\n",
       " '',\n",
       " '    scala> spark.range(1000 * 1000 * 1000).count()',\n",
       " '',\n",
       " '## Interactive Python Shell',\n",
       " '',\n",
       " 'Alternatively, if you prefer Python, you can use the Python shell:',\n",
       " '',\n",
       " '    ./bin/pyspark',\n",
       " '',\n",
       " 'And run the following command, which should also return 1,000,000,000:',\n",
       " '',\n",
       " '    >>> spark.range(1000 * 1000 * 1000).count()',\n",
       " '',\n",
       " '## Example Programs',\n",
       " '',\n",
       " 'Spark also comes with several sample programs in the `examples` directory.',\n",
       " 'To run one of them, use `./bin/run-example <class> [params]`. For example:',\n",
       " '',\n",
       " '    ./bin/run-example SparkPi',\n",
       " '',\n",
       " 'will run the Pi example locally.',\n",
       " '',\n",
       " 'You can set the MASTER environment variable when running examples to submit',\n",
       " 'examples to a cluster. This can be a mesos:// or spark:// URL,',\n",
       " '\"yarn\" to run on YARN, and \"local\" to run',\n",
       " 'locally with one thread, or \"local[N]\" to run locally with N threads. You',\n",
       " 'can also use an abbreviated class name if the class is in the `examples`',\n",
       " 'package. For instance:',\n",
       " '',\n",
       " '    MASTER=spark://host:7077 ./bin/run-example SparkPi',\n",
       " '',\n",
       " 'Many of the example programs print usage help if no params are given.',\n",
       " '',\n",
       " '## Running Tests',\n",
       " '',\n",
       " 'Testing first requires [building Spark](#building-spark). Once Spark is built, tests',\n",
       " 'can be run using:',\n",
       " '',\n",
       " '    ./dev/run-tests',\n",
       " '',\n",
       " 'Please see the guidance on how to',\n",
       " '[run tests for a module, or individual tests](https://spark.apache.org/developer-tools.html#individual-tests).',\n",
       " '',\n",
       " 'There is also a Kubernetes integration test, see resource-managers/kubernetes/integration-tests/README.md',\n",
       " '',\n",
       " '## A Note About Hadoop Versions',\n",
       " '',\n",
       " 'Spark uses the Hadoop core library to talk to HDFS and other Hadoop-supported',\n",
       " 'storage systems. Because the protocols have changed in different versions of',\n",
       " 'Hadoop, you must build Spark against the same version that your cluster runs.',\n",
       " '',\n",
       " 'Please refer to the build documentation at',\n",
       " '[\"Specifying the Hadoop Version and Enabling YARN\"](https://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version-and-enabling-yarn)',\n",
       " 'for detailed guidance on building for a particular distribution of Hadoop, including',\n",
       " 'building for particular Hive and Hive Thriftserver distributions.',\n",
       " '',\n",
       " '## Configuration',\n",
       " '',\n",
       " 'Please refer to the [Configuration Guide](https://spark.apache.org/docs/latest/configuration.html)',\n",
       " 'in the online documentation for an overview on how to configure Spark.',\n",
       " '',\n",
       " '## Contributing',\n",
       " '',\n",
       " 'Please review the [Contribution to Spark guide](https://spark.apache.org/contributing.html)',\n",
       " 'for information on how to get started contributing to the project.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['high-level APIs in Scala, Java, Python, and R, and an optimized engine that',\n",
       " '## Interactive Python Shell',\n",
       " 'Alternatively, if you prefer Python, you can use the Python shell:']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "python_in_lines = lines.filter(lambda line: \"Python\" in line)\n",
    "python_in_lines.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('# Apache Spark', 1),\n",
       " ('', 1),\n",
       " ('Spark is a unified analytics engine for large-scale data processing. It provides',\n",
       "  1),\n",
       " ('high-level APIs in Scala, Java, Python, and R, and an optimized engine that',\n",
       "  1),\n",
       " ('supports general computation graphs for data analysis. It also supports a',\n",
       "  1),\n",
       " ('rich set of higher-level tools including Spark SQL for SQL and DataFrames,',\n",
       "  1),\n",
       " ('MLlib for machine learning, GraphX for graph processing,', 1),\n",
       " ('and Structured Streaming for stream processing.', 1),\n",
       " ('', 1),\n",
       " ('<https://spark.apache.org/>', 1),\n",
       " ('', 1),\n",
       " ('[![Jenkins Build](https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-sbt-hadoop-2.7-hive-2.3/badge/icon)](https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-sbt-hadoop-2.7-hive-2.3)',\n",
       "  1),\n",
       " ('[![AppVeyor Build](https://img.shields.io/appveyor/ci/ApacheSoftwareFoundation/spark/master.svg?style=plastic&logo=appveyor)](https://ci.appveyor.com/project/ApacheSoftwareFoundation/spark)',\n",
       "  1),\n",
       " ('[![PySpark Coverage](https://img.shields.io/badge/dynamic/xml.svg?label=pyspark%20coverage&url=https%3A%2F%2Fspark-test.github.io%2Fpyspark-coverage-site&query=%2Fhtml%2Fbody%2Fdiv%5B1%5D%2Fdiv%2Fh1%2Fspan&colorB=brightgreen&style=plastic)](https://spark-test.github.io/pyspark-coverage-site)',\n",
       "  1),\n",
       " ('', 1),\n",
       " ('', 1),\n",
       " ('## Online Documentation', 1),\n",
       " ('', 1),\n",
       " ('You can find the latest Spark documentation, including a programming', 1),\n",
       " ('guide, on the [project web page](https://spark.apache.org/documentation.html).',\n",
       "  1),\n",
       " ('This README file only contains basic setup instructions.', 1),\n",
       " ('', 1),\n",
       " ('## Building Spark', 1),\n",
       " ('', 1),\n",
       " ('Spark is built using [Apache Maven](https://maven.apache.org/).', 1),\n",
       " ('To build Spark and its example programs, run:', 1),\n",
       " ('', 1),\n",
       " ('    ./build/mvn -DskipTests clean package', 1),\n",
       " ('', 1),\n",
       " ('(You do not need to do this if you downloaded a pre-built package.)', 1),\n",
       " ('', 1),\n",
       " ('More detailed documentation is available from the project site, at', 1),\n",
       " ('[\"Building Spark\"](https://spark.apache.org/docs/latest/building-spark.html).',\n",
       "  1),\n",
       " ('', 1),\n",
       " ('For general development tips, including info on developing Spark using an IDE, see [\"Useful Developer Tools\"](https://spark.apache.org/developer-tools.html).',\n",
       "  1),\n",
       " ('', 1),\n",
       " ('## Interactive Scala Shell', 1),\n",
       " ('', 1),\n",
       " ('The easiest way to start using Spark is through the Scala shell:', 1),\n",
       " ('', 1),\n",
       " ('    ./bin/spark-shell', 1),\n",
       " ('', 1),\n",
       " ('Try the following command, which should return 1,000,000,000:', 1),\n",
       " ('', 1),\n",
       " ('    scala> spark.range(1000 * 1000 * 1000).count()', 1),\n",
       " ('', 1),\n",
       " ('## Interactive Python Shell', 1),\n",
       " ('', 1),\n",
       " ('Alternatively, if you prefer Python, you can use the Python shell:', 1),\n",
       " ('', 1),\n",
       " ('    ./bin/pyspark', 1),\n",
       " ('', 1),\n",
       " ('And run the following command, which should also return 1,000,000,000:', 1),\n",
       " ('', 1),\n",
       " ('    >>> spark.range(1000 * 1000 * 1000).count()', 1),\n",
       " ('', 1),\n",
       " ('## Example Programs', 1),\n",
       " ('', 1),\n",
       " ('Spark also comes with several sample programs in the `examples` directory.',\n",
       "  1),\n",
       " ('To run one of them, use `./bin/run-example <class> [params]`. For example:',\n",
       "  1),\n",
       " ('', 1),\n",
       " ('    ./bin/run-example SparkPi', 1),\n",
       " ('', 1),\n",
       " ('will run the Pi example locally.', 1),\n",
       " ('', 1),\n",
       " ('You can set the MASTER environment variable when running examples to submit',\n",
       "  1),\n",
       " ('examples to a cluster. This can be a mesos:// or spark:// URL,', 1),\n",
       " ('\"yarn\" to run on YARN, and \"local\" to run', 1),\n",
       " ('locally with one thread, or \"local[N]\" to run locally with N threads. You',\n",
       "  1),\n",
       " ('can also use an abbreviated class name if the class is in the `examples`',\n",
       "  1),\n",
       " ('package. For instance:', 1),\n",
       " ('', 1),\n",
       " ('    MASTER=spark://host:7077 ./bin/run-example SparkPi', 1),\n",
       " ('', 1),\n",
       " ('Many of the example programs print usage help if no params are given.', 1),\n",
       " ('', 1),\n",
       " ('## Running Tests', 1),\n",
       " ('', 1),\n",
       " ('Testing first requires [building Spark](#building-spark). Once Spark is built, tests',\n",
       "  1),\n",
       " ('can be run using:', 1),\n",
       " ('', 1),\n",
       " ('    ./dev/run-tests', 1),\n",
       " ('', 1),\n",
       " ('Please see the guidance on how to', 1),\n",
       " ('[run tests for a module, or individual tests](https://spark.apache.org/developer-tools.html#individual-tests).',\n",
       "  1),\n",
       " ('', 1),\n",
       " ('There is also a Kubernetes integration test, see resource-managers/kubernetes/integration-tests/README.md',\n",
       "  1),\n",
       " ('', 1),\n",
       " ('## A Note About Hadoop Versions', 1),\n",
       " ('', 1),\n",
       " ('Spark uses the Hadoop core library to talk to HDFS and other Hadoop-supported',\n",
       "  1),\n",
       " ('storage systems. Because the protocols have changed in different versions of',\n",
       "  1),\n",
       " ('Hadoop, you must build Spark against the same version that your cluster runs.',\n",
       "  1),\n",
       " ('', 1),\n",
       " ('Please refer to the build documentation at', 1),\n",
       " ('[\"Specifying the Hadoop Version and Enabling YARN\"](https://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version-and-enabling-yarn)',\n",
       "  1),\n",
       " ('for detailed guidance on building for a particular distribution of Hadoop, including',\n",
       "  1),\n",
       " ('building for particular Hive and Hive Thriftserver distributions.', 1),\n",
       " ('', 1),\n",
       " ('## Configuration', 1),\n",
       " ('', 1),\n",
       " ('Please refer to the [Configuration Guide](https://spark.apache.org/docs/latest/configuration.html)',\n",
       "  1),\n",
       " ('in the online documentation for an overview on how to configure Spark.', 1),\n",
       " ('', 1),\n",
       " ('## Contributing', 1),\n",
       " ('', 1),\n",
       " ('Please review the [Contribution to Spark guide](https://spark.apache.org/contributing.html)',\n",
       "  1),\n",
       " ('for information on how to get started contributing to the project.', 1)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.textFile(\"README.md\")\n",
    "rdd_map= rdd.map(lambda x: (x,1))\n",
    "rdd_map.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['# Apache Spark',\n",
       " '',\n",
       " 'Spark is a unified analytics engine for large-scale data processing. It provides',\n",
       " 'high-level APIs in Scala, Java, Python, and R, and an optimized engine that',\n",
       " 'supports general computation graphs for data analysis. It also supports a']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mylist = [1,2,3,4,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRDD = spark.sparkContext.parallelize(mylist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "myRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(\"James\", \"Smith\", \"1991-04-01\", \"M\", 3000), (\"Michael\", \"Rose\", \"2000-05-19\", \"M\", 4000), (\"Robert\", \"Williams\", \"1978-09-05\", \"M\", 4000), (\"Maria\", \"Anne\", \"1994-11-09\", \"F\", 4000), (\"Jen\", \"Mary\", \"1980-02-17\", \"F\", 5000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"firstname\", \"lastname\", \"date_of_birth\", \"gender\", \"salary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(data=data, schema=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.DataStreamReader at 0x248a68cf2e0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- date_of_birth: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------------+------+------+\n",
      "|firstname|lastname|date_of_birth|gender|salary|\n",
      "+---------+--------+-------------+------+------+\n",
      "|    James|   Smith|   1991-04-01|     M|  3000|\n",
      "|  Michael|    Rose|   2000-05-19|     M|  4000|\n",
      "|   Robert|Williams|   1978-09-05|     M|  4000|\n",
      "|    Maria|    Anne|   1994-11-09|     F|  4000|\n",
      "|      Jen|    Mary|   1980-02-17|     F|  5000|\n",
      "+---------+--------+-------------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected character after line continuation character (<ipython-input-2-4e906fce5663>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-4e906fce5663>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    .format(\"kafka\") \\\u001b[0m\n\u001b[1;37m                       \n^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected character after line continuation character\n"
     ]
    }
   ],
   "source": [
    "# structued streaming\n",
    "df = spark.readStream \n",
    "        .format(\"kafka\") \n",
    "        .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \n",
    "        .option(\"subscribe\", \"json_topic\")\n",
    "        .option(\"startingOffsets\", \"earliest\") \n",
    "        .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
